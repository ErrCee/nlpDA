{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlyR393dRWs5Pk27FXM6g7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Normalization**"],"metadata":{"id":"9Z5mA9UMPhzr"}},{"cell_type":"code","source":["import pathlib\n","import pickle\n","import random\n","import re\n","import unicodedata\n","\n","import tensorflow as tf\n","\n","text_file = tf.keras.utils.get_file(\n","    fname=\"fra-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n","    extract=True,\n",")\n","text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n","\n","def normalize(line):\n","    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n","    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n","    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n","    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n","    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n","    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n","    eng, fra = line.split(\"\\t\")\n","    fra = \"[start] \" + fra + \" [end]\"\n","    return eng, fra\n","\n","# normalize each line and separate into English and French\n","with open(text_file) as fp:\n","    text_pairs = [normalize(line) for line in fp]\n","\n","# print some samples\n","for _ in range(5):\n","    print(random.choice(text_pairs))\n","\n","with open(\"text_pairs.pickle\", \"wb\") as fp:\n","    pickle.dump(text_pairs, fp)\n","\n","with open(\"text_pairs.pickle\", \"rb\") as fp:\n","    text_pairs = pickle.load(fp)\n","\n","# count tokens\n","eng_tokens, fra_tokens = set(), set()\n","eng_maxlen, fra_maxlen = 0, 0\n","for eng, fra in text_pairs:\n","    eng_tok, fra_tok = eng.split(), fra.split()\n","    eng_maxlen = max(eng_maxlen, len(eng_tok))\n","    fra_maxlen = max(fra_maxlen, len(fra_tok))\n","    eng_tokens.update(eng_tok)\n","    fra_tokens.update(fra_tok)\n","print(f\"Total English tokens: {len(eng_tokens)}\")\n","print(f\"Total French tokens: {len(fra_tokens)}\")\n","print(f\"Max English length: {eng_maxlen}\")\n","print(f\"Max French length: {fra_maxlen}\")\n","print(f\"{len(text_pairs)} total pairs\")"],"metadata":{"id":"k9AUieZ0FU91"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Vectorization**"],"metadata":{"id":"ghX3xBGIPmk5"}},{"cell_type":"code","source":["import pickle\n","import random\n","\n","from tensorflow.keras.layers import TextVectorization\n","\n","# Load normalized sentence pairs\n","with open(\"text_pairs.pickle\", \"rb\") as fp:\n","    text_pairs = pickle.load(fp)\n","\n","# train-test-val split of randomized sentence pairs\n","random.shuffle(text_pairs)\n","n_val = int(0.15*len(text_pairs))\n","n_train = len(text_pairs) - 2*n_val\n","train_pairs = text_pairs[:n_train]\n","val_pairs = text_pairs[n_train:n_train+n_val]\n","test_pairs = text_pairs[n_train+n_val:]\n","\n","# Parameter determined after analyzing the input data\n","vocab_size_en = 10000\n","vocab_size_fr = 20000\n","seq_length = 20\n","\n","# Create vectorizer\n","eng_vectorizer = TextVectorization(\n","    max_tokens=vocab_size_en,\n","    standardize=None,\n","    split=\"whitespace\",\n","    output_mode=\"int\",\n","    output_sequence_length=seq_length,\n",")\n","fra_vectorizer = TextVectorization(\n","    max_tokens=vocab_size_fr,\n","    standardize=None,\n","    split=\"whitespace\",\n","    output_mode=\"int\",\n","    output_sequence_length=seq_length + 1\n",")\n","\n","# train the vectorization layer using training dataset\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_fra_texts = [pair[1] for pair in train_pairs]\n","eng_vectorizer.adapt(train_eng_texts)\n","fra_vectorizer.adapt(train_fra_texts)\n","\n","# save for subsequent steps\n","with open(\"vectorize.pickle\", \"wb\") as fp:\n","    data = {\n","        \"train\": train_pairs,\n","        \"val\":   val_pairs,\n","        \"test\":  test_pairs,\n","        \"engvec_config\":  eng_vectorizer.get_config(),\n","        \"engvec_weights\": eng_vectorizer.get_weights(),\n","        \"fravec_config\":  fra_vectorizer.get_config(),\n","        \"fravec_weights\": fra_vectorizer.get_weights(),\n","    }\n","    pickle.dump(data, fp)"],"metadata":{"id":"cM4NQ4s8FfBA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Positional Encoding**"],"metadata":{"id":"9qNnHZaKPtAG"}},{"cell_type":"code","source":["import pickle\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","\n","# load text data and vectorizer weights\n","with open(\"vectorize.pickle\", \"rb\") as fp:\n","    data = pickle.load(fp)\n","\n","train_pairs = data[\"train\"]\n","val_pairs = data[\"val\"]\n","test_pairs = data[\"test\"]   # not used\n","\n","eng_vectorizer = TextVectorization.from_config(data[\"engvec_config\"])\n","eng_vectorizer.set_weights(data[\"engvec_weights\"])\n","fra_vectorizer = TextVectorization.from_config(data[\"fravec_config\"])\n","fra_vectorizer.set_weights(data[\"fravec_weights\"])\n","\n","# set up Dataset object\n","def format_dataset(eng, fra):\n","    \"\"\"Take an English and a French sentence pair, convert into input and target.\n","    The input is a dict with keys `encoder_inputs` and `decoder_inputs`, each\n","    is a vector, corresponding to English and French sentences respectively.\n","    The target is also vector of the French sentence, advanced by 1 token. All\n","    vector are in the same length.\n","\n","    The output will be used for training the transformer model. In the model we\n","    will create, the input tensors are named `encoder_inputs` and `decoder_inputs`\n","    which should be matched to the keys in the dictionary for the source part\n","    \"\"\"\n","    eng = eng_vectorizer(eng)\n","    fra = fra_vectorizer(fra)\n","    source = {\"encoder_inputs\": eng,\n","              \"decoder_inputs\": fra[:, :-1]}\n","    target = fra[:, 1:]\n","    return (source, target)\n","\n","def make_dataset(pairs, batch_size=64):\n","    \"\"\"Create TensorFlow Dataset for the sentence pairs\"\"\"\n","    # aggregate sentences using zip(*pairs)\n","    eng_texts, fra_texts = zip(*pairs)\n","    # convert them into list, and then create tensors\n","    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(fra_texts)))\n","    return dataset.shuffle(2048) \\\n","                  .batch(batch_size).map(format_dataset) \\\n","                  .prefetch(16).cache()\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)\n","\n","# test the dataset\n","for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"encoder_inputs\"][0]: {inputs[\"encoder_inputs\"][0]}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"][0]: {inputs[\"decoder_inputs\"][0]}')\n","    print(f\"targets.shape: {targets.shape}\")\n","    print(f\"targets[0]: {targets[0]}\")"],"metadata":{"id":"IoOx50mgFZgv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZMPesJbpGW2C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Encoding Layer**"],"metadata":{"id":"nv5bI0SMGpvI"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","def pos_enc_matrix(L, d, n=10000):\n","    \"\"\"Create positional encoding matrix\n","\n","    Args:\n","        L: Input dimension (length)\n","        d: Output dimension (depth), even only\n","        n: Constant for the sinusoidal functions\n","\n","    Returns:\n","        numpy matrix of floats of dimension L-by-d. At element (k,2i) the value\n","        is sin(k/n^(2i/d)) while at element (k,2i+1) the value is cos(k/n^(2i/d))\n","    \"\"\"\n","    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n","    d2 = d//2\n","    P = np.zeros((L, d))\n","    k = np.arange(L).reshape(-1, 1)     # L-column vector\n","    i = np.arange(d2).reshape(1, -1)    # d-row vector\n","    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n","    args = k * denom                    # (L,d) matrix\n","    P[:, ::2] = np.sin(args)\n","    P[:, 1::2] = np.cos(args)\n","    return P\n","\n","class PositionalEmbedding(tf.keras.layers.Layer):\n","    \"\"\"Positional embedding layer. Assume tokenized input, transform into\n","    embedding and returns positional-encoded output.\"\"\"\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        \"\"\"\n","        Args:\n","            sequence_length: Input sequence length\n","            vocab_size: Input vocab size, for setting up embedding matrix\n","            embed_dim: Embedding vector size, for setting up embedding matrix\n","        \"\"\"\n","        super().__init__(**kwargs)\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim     # d_model in paper\n","        # token embedding layer: Convert integer token to D-dim float vector\n","        self.token_embeddings = tf.keras.layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim, mask_zero=True\n","        )\n","        # positional embedding layer: a matrix of hard-coded sine values\n","        matrix = pos_enc_matrix(sequence_length, embed_dim)\n","        self.position_embeddings = tf.constant(matrix, dtype=\"float32\")\n","\n","    def call(self, inputs):\n","        \"\"\"Input tokens convert into embedding vectors then superimposed\n","        with position vectors\"\"\"\n","        embedded_tokens = self.token_embeddings(inputs)\n","        return embedded_tokens + self.position_embeddings\n","\n","    # this layer is using an Embedding layer, which can take a mask\n","    # see https://www.tensorflow.org/guide/keras/masking_and_padding#passing_mask_tensors_directly_to_layers\n","    def compute_mask(self, *args, **kwargs):\n","        return self.token_embeddings.compute_mask(*args, **kwargs)\n","\n","    def get_config(self):\n","        # to make save and load a model using custom layer possible\n","        config = super().get_config()\n","        config.update({\n","            \"sequence_length\": self.sequence_length,\n","            \"vocab_size\": self.vocab_size,\n","            \"embed_dim\": self.embed_dim,\n","        })\n","        return config"],"metadata":{"id":"oSJc7rzyGu2h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Self Attention**"],"metadata":{"id":"JKFPGoA6HABF"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def self_attention(input_shape, prefix=\"att\", mask=False, **kwargs):\n","    \"\"\"Self-attention layers at transformer encoder and decoder. Assumes its\n","    input is the output from positional encoding layer.\n","\n","    Args:\n","        prefix (str): The prefix added to the layer names\n","        masked (bool): whether to use causal mask. Should be False on encoder and\n","                       True on decoder. When True, a mask will be applied such that\n","                       each location only has access to the locations before it.\n","    \"\"\"\n","    # create layers\n","    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n","                                   name=f\"{prefix}_in1\")\n","    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn1\", **kwargs)\n","    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm1\")\n","    add = tf.keras.layers.Add(name=f\"{prefix}_add1\")\n","    # functional API to connect input to output\n","    attout = attention(query=inputs, value=inputs, key=inputs,\n","                       use_causal_mask=mask)\n","    outputs = norm(add([inputs, attout]))\n","    # create model and return\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_att\")\n","    return model\n","\n","seq_length = 20\n","key_dim = 128\n","num_heads = 8\n","\n","model = self_attention(input_shape=(seq_length, key_dim),\n","                       num_heads=num_heads, key_dim=key_dim)\n","tf.keras.utils.plot_model(model, \"self-attention.png\",\n","                          show_shapes=True, show_dtype=True, show_layer_names=True,\n","                          rankdir='BT', show_layer_activations=True)"],"metadata":{"id":"hSCmU1itGzb8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cross Attention**"],"metadata":{"id":"C34jxET2HGvp"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def cross_attention(input_shape, context_shape, prefix=\"att\", **kwargs):\n","    \"\"\"Cross-attention layers at transformer decoder. Assumes its\n","    input is the output from positional encoding layer at decoder\n","    and context is the final output from encoder.\n","\n","    Args:\n","        prefix (str): The prefix added to the layer names\n","    \"\"\"\n","    # create layers\n","    context = tf.keras.layers.Input(shape=context_shape, dtype='float32',\n","                                    name=f\"{prefix}_ctx2\")\n","    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n","                                   name=f\"{prefix}_in2\")\n","    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn2\", **kwargs)\n","    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm2\")\n","    add = tf.keras.layers.Add(name=f\"{prefix}_add2\")\n","    # functional API to connect input to output\n","    attout = attention(query=inputs, value=context, key=context)\n","    outputs = norm(add([attout, inputs]))\n","    # create model and return\n","    model = tf.keras.Model(inputs=[(context, inputs)], outputs=outputs,\n","                           name=f\"{prefix}_cross\")\n","    return model\n","\n","seq_length = 20\n","key_dim = 128\n","num_heads = 8\n","\n","model = cross_attention(input_shape=(seq_length, key_dim),\n","                        context_shape=(seq_length, key_dim),\n","                        num_heads=num_heads, key_dim=key_dim)\n","tf.keras.utils.plot_model(model, \"cross-attention.png\",\n","                          show_shapes=True, show_dtype=True, show_layer_names=True,\n","                          rankdir='BT', show_layer_activations=True)"],"metadata":{"id":"-lbOpcZNHNKY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Feed Forward**"],"metadata":{"id":"o9eZAcXuHSGI"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def feed_forward(input_shape, model_dim, ff_dim, dropout=0.1, prefix=\"ff\"):\n","    \"\"\"Feed-forward layers at transformer encoder and decoder. Assumes its\n","    input is the output from an attention layer with add & norm, the output\n","    is the output of one encoder or decoder block\n","\n","    Args:\n","        model_dim (int): Output dimension of the feed-forward layer, which\n","                         is also the output dimension of the encoder/decoder\n","                         block\n","        ff_dim (int): Internal dimension of the feed-forward layer\n","        dropout (float): Dropout rate\n","        prefix (str): The prefix added to the layer names\n","    \"\"\"\n","    # create layers\n","    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n","                                   name=f\"{prefix}_in3\")\n","    dense1 = tf.keras.layers.Dense(ff_dim, name=f\"{prefix}_ff1\", activation=\"relu\")\n","    dense2 = tf.keras.layers.Dense(model_dim, name=f\"{prefix}_ff2\")\n","    drop = tf.keras.layers.Dropout(dropout, name=f\"{prefix}_drop\")\n","    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n","    # functional API to connect input to output\n","    ffout = drop(dense2(dense1(inputs)))\n","    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm3\")\n","    outputs = norm(add([inputs, ffout]))\n","    # create model and return\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_ff\")\n","    return model\n","\n","seq_length = 20\n","key_dim = 128\n","ff_dim = 512\n","\n","model = feed_forward(input_shape=(seq_length, key_dim),\n","                     model_dim=key_dim, ff_dim=ff_dim)\n","tf.keras.utils.plot_model(model, \"feedforward.png\",\n","                          show_shapes=True, show_dtype=True, show_layer_names=True,\n","                          rankdir='BT', show_layer_activations=True)"],"metadata":{"id":"o5Tq02C2HcqC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Encoder**"],"metadata":{"id":"KKiO-Rf_HeGM"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def feed_forward(input_shape, model_dim, ff_dim, dropout=0.1, prefix=\"ff\"):\n","    \"\"\"Feed-forward layers at transformer encoder and decoder. Assumes its\n","    input is the output from an attention layer with add & norm, the output\n","    is the output of one encoder or decoder block\n","\n","    Args:\n","        model_dim (int): Output dimension of the feed-forward layer, which\n","                         is also the output dimension of the encoder/decoder\n","                         block\n","        ff_dim (int): Internal dimension of the feed-forward layer\n","        dropout (float): Dropout rate\n","        prefix (str): The prefix added to the layer names\n","    \"\"\"\n","    # create layers\n","    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n","                                   name=f\"{prefix}_in3\")\n","    dense1 = tf.keras.layers.Dense(ff_dim, name=f\"{prefix}_ff1\", activation=\"relu\")\n","    dense2 = tf.keras.layers.Dense(model_dim, name=f\"{prefix}_ff2\")\n","    drop = tf.keras.layers.Dropout(dropout, name=f\"{prefix}_drop\")\n","    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n","    # functional API to connect input to output\n","    ffout = drop(dense2(dense1(inputs)))\n","    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm3\")\n","    outputs = norm(add([inputs, ffout]))\n","    # create model and return\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_ff\")\n","    return model\n","\n","seq_length = 20\n","key_dim = 128\n","ff_dim = 512\n","\n","model = feed_forward(input_shape=(seq_length, key_dim),\n","                     model_dim=key_dim, ff_dim=ff_dim)\n","tf.keras.utils.plot_model(model, \"feedforward.png\",\n","                          show_shapes=True, show_dtype=True, show_layer_names=True,\n","                          rankdir='BT', show_layer_activations=True)"],"metadata":{"id":"hTcqo0OvHkAT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Decoder**"],"metadata":{"id":"iqmogyriHmbo"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# the three building block functions from Lesson 06\n","from lesson_06 import self_attention, cross_attention, feed_forward\n","\n","def decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"dec\", **kwargs):\n","    \"\"\"One decoder unit. The input and output are in the same shape so we can\n","    daisy chain multiple decoder units into one larger decoder. The context\n","    vector is also assumed to be the same shape for convenience\"\"\"\n","    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n","                                   name=f\"{prefix}_in0\")\n","    context = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n","                                    name=f\"{prefix}_ctx0\")\n","    attmodel = self_attention(input_shape, key_dim=key_dim, mask=True,\n","                              prefix=prefix, **kwargs)\n","    crossmodel = cross_attention(input_shape, input_shape, key_dim=key_dim,\n","                                 prefix=prefix, **kwargs)\n","    ffmodel = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n","    x = attmodel(inputs)\n","    x = crossmodel([(context, x)])\n","    output = ffmodel(x)\n","    model = tf.keras.Model(inputs=[(inputs, context)], outputs=output, name=prefix)\n","    return model\n","\n","\n","seq_length = 20\n","key_dim = 128\n","ff_dim = 512\n","num_heads = 8\n","\n","model = decoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n","                num_heads=num_heads)\n","tf.keras.utils.plot_model(model, \"decoder.png\",\n","                          show_shapes=True, show_dtype=True, show_layer_names=True,\n","                          rankdir='BT', show_layer_activations=True)"],"metadata":{"id":"Z5xdeYuhHxoS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Transformer**"],"metadata":{"id":"7gNpZoFIHzF2"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# the positional embedding layer from Lesson 05\n","from lesson_05 import PositionalEmbedding\n","# the building block functions from Lesson 07\n","from lesson_07 import encoder, decoder\n","\n","\n","def transformer(num_layers, num_heads, seq_len, key_dim, ff_dim, vocab_size_src,\n","                vocab_size_tgt, dropout=0.1, name=\"transformer\"):\n","    embed_shape = (seq_len, key_dim)  # output shape of the positional embedding layer\n","    # set up layers\n","    input_enc = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n","                                      name=\"encoder_inputs\")\n","    input_dec = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n","                                      name=\"decoder_inputs\")\n","    embed_enc = PositionalEmbedding(seq_len, vocab_size_src, key_dim, name=\"embed_enc\")\n","    embed_dec = PositionalEmbedding(seq_len, vocab_size_tgt, key_dim, name=\"embed_dec\")\n","    encoders = [encoder(input_shape=embed_shape, key_dim=key_dim,\n","                        ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\",\n","                        num_heads=num_heads)\n","                for i in range(num_layers)]\n","    decoders = [decoder(input_shape=embed_shape, key_dim=key_dim,\n","                        ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\",\n","                        num_heads=num_heads)\n","                for i in range(num_layers)]\n","    final = tf.keras.layers.Dense(vocab_size_tgt, name=\"linear\")\n","    # build output\n","    x1 = embed_enc(input_enc)\n","    x2 = embed_dec(input_dec)\n","    for layer in encoders:\n","        x1 = layer(x1)\n","    for layer in decoders:\n","        x2 = layer([x2, x1])\n","    output = final(x2)\n","    # XXX keep this try-except block\n","    try:\n","        del output._keras_mask\n","    except AttributeError:\n","        pass\n","    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n","    return model\n","\n","\n","seq_len = 20\n","num_layers = 4\n","num_heads = 8\n","key_dim = 128\n","ff_dim = 512\n","dropout = 0.1\n","vocab_size_en = 10000\n","vocab_size_fr = 20000\n","model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n","                    vocab_size_en, vocab_size_fr, dropout)\n","tf.keras.utils.plot_model(model, \"transformer.png\",\n","                          show_shapes=True, show_dtype=True, show_layer_names=True,\n","                          rankdir='BT', show_layer_activations=True)"],"metadata":{"id":"DKV6q-QbH5oV"},"execution_count":null,"outputs":[]}]}